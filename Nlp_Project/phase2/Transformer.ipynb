{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8f91c0-a4ce-43d6-b49e-9d54c6bf9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c24dc0-5f29-4231-8338-bb54e1558fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in c:\\users\\admin\\anaconda3\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\lib\\site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\admin\\anaconda3\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\admin\\anaconda3\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from evaluate) (0.33.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d51ee7ea-3dbb-411c-87e7-799afb719066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from evaluate import load  # âœ… Ø§Ù„Ø­Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f74b801-3156-4373-bde9-c5f1b183dffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b44925-73d9-4fda-bc34-a2f6475b6726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece  # ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„ØªØ­Ù…ÙŠÙ„ ØªÙ…\n",
    "\n",
    "model_ckpt = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "\n",
    "# Ø­Ù…Ù„ Ø§Ù„ØªÙˆÙƒÙ†Ù€Ø§ÙŠØ²Ø±\n",
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "# Ø­Ù…Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙØ¹Ù„ÙŠ\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df806df4-d7c7-4220-a902-542897160706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d41b30cf1c74ff181a107a8f29386fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  34%|###4      | 1.21G/3.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_mT5_multilingual_XLSum = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9e31baf-d52b-4284-8d49-b1868b5875f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Column2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summary</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ØªÙ†Ø§ÙˆÙ„ Ø§Ù„ÙØ§ÙƒÙ‡Ø© ÙˆØ§Ù„Ø®Ø¶Ø±Ø§ÙˆØ§Øª ÙÙŠ Ù…ÙˆØ³Ù…Ù‡Ø§. ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø£...</td>\n",
       "      <td>ÙŠÙƒÙˆÙ† Ø³Ø¹Ø± Ø§Ù„ÙØ§ÙƒÙ‡Ø© ÙˆØ§Ù„Ø®Ø¶Ø±Ø§ÙˆØ§Øª ÙÙŠ Ù…ÙˆØ³Ù… Ø¥Ù†Ø¨Ø§ØªÙ‡Ø§ Ø£Ù‚...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ÙØ¶Ù„ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø£Ø·Ø¹Ù…Ø© Ø§Ù„Ø£Ø±Ø®Øµ Ø«Ù…Ù†Ø§. ØªØ¨Ø§Ø·Ø£ ÙÙŠ Ø§Ø³ØªÙ‡Ù„...</td>\n",
       "      <td>Ø§Ù„Ø£Ø·Ø¹Ù…Ø© Ø§Ù„ØµØ­ÙŠØ© Ù„ÙŠØ³Øª Ø¨Ø§Ù‡Ø¸Ø© Ø§Ù„Ø«Ù…Ù† Ø¨Ø§Ù„Ø¶Ø±ÙˆØ±Ø©ØŒ Ø¨Ù„ Ù...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø§Ø²Ø±Ø¹ ÙƒÙ„ Ù…Ø§ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ù† Ø®Ø¶Ø±Ø§ÙˆØ§Øª ÙˆÙØ§ÙƒÙ‡Ø©. Ø§Ø·Ù‡ Ø¨Ù†ÙØ³Ùƒ ...</td>\n",
       "      <td>Ø§Ø³ØªÙØ¯ Ù…Ù† Ø­Ø¯ÙŠÙ‚ØªÙƒ Ø§Ù„Ù…Ù†Ø²Ù„ÙŠØ© Ø£Ùˆ Ø£ØµÙŠØµ Ø§Ù„Ø²Ø±Ø¹ Ø§Ù„ØµØºÙŠØ± ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø®Ø·Ø· Ù…Ø³Ø¨Ù‚Ø§ Ù„ÙˆØ¬Ø¨Ø§ØªÙƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù…Ø¯Ø© Ø£Ø³Ø¨ÙˆØ¹. Ø§Ù„ØªØ²Ù… Ø¨...</td>\n",
       "      <td>ØªØ³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø®Ø·Ø· Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© ÙÙŠ ÙƒÙ„ Ù†ÙˆØ§Ø­ÙŠ Ø­ÙŠØ§ØªÙƒ Ø¹Ù„Ù‰ ÙˆØ¶Ø¹...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Column1  \\\n",
       "0                                            summary   \n",
       "1  ØªÙ†Ø§ÙˆÙ„ Ø§Ù„ÙØ§ÙƒÙ‡Ø© ÙˆØ§Ù„Ø®Ø¶Ø±Ø§ÙˆØ§Øª ÙÙŠ Ù…ÙˆØ³Ù…Ù‡Ø§. ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø£...   \n",
       "2  ÙØ¶Ù„ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø£Ø·Ø¹Ù…Ø© Ø§Ù„Ø£Ø±Ø®Øµ Ø«Ù…Ù†Ø§. ØªØ¨Ø§Ø·Ø£ ÙÙŠ Ø§Ø³ØªÙ‡Ù„...   \n",
       "3  Ø§Ø²Ø±Ø¹ ÙƒÙ„ Ù…Ø§ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ù† Ø®Ø¶Ø±Ø§ÙˆØ§Øª ÙˆÙØ§ÙƒÙ‡Ø©. Ø§Ø·Ù‡ Ø¨Ù†ÙØ³Ùƒ ...   \n",
       "4  Ø®Ø·Ø· Ù…Ø³Ø¨Ù‚Ø§ Ù„ÙˆØ¬Ø¨Ø§ØªÙƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù…Ø¯Ø© Ø£Ø³Ø¨ÙˆØ¹. Ø§Ù„ØªØ²Ù… Ø¨...   \n",
       "\n",
       "                                             Column2  \n",
       "0                                               text  \n",
       "1  ÙŠÙƒÙˆÙ† Ø³Ø¹Ø± Ø§Ù„ÙØ§ÙƒÙ‡Ø© ÙˆØ§Ù„Ø®Ø¶Ø±Ø§ÙˆØ§Øª ÙÙŠ Ù…ÙˆØ³Ù… Ø¥Ù†Ø¨Ø§ØªÙ‡Ø§ Ø£Ù‚...  \n",
       "2  Ø§Ù„Ø£Ø·Ø¹Ù…Ø© Ø§Ù„ØµØ­ÙŠØ© Ù„ÙŠØ³Øª Ø¨Ø§Ù‡Ø¸Ø© Ø§Ù„Ø«Ù…Ù† Ø¨Ø§Ù„Ø¶Ø±ÙˆØ±Ø©ØŒ Ø¨Ù„ Ù...  \n",
       "3  Ø§Ø³ØªÙØ¯ Ù…Ù† Ø­Ø¯ÙŠÙ‚ØªÙƒ Ø§Ù„Ù…Ù†Ø²Ù„ÙŠØ© Ø£Ùˆ Ø£ØµÙŠØµ Ø§Ù„Ø²Ø±Ø¹ Ø§Ù„ØµØºÙŠØ± ...  \n",
       "4  ØªØ³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø®Ø·Ø· Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© ÙÙŠ ÙƒÙ„ Ù†ÙˆØ§Ø­ÙŠ Ø­ÙŠØ§ØªÙƒ Ø¹Ù„Ù‰ ÙˆØ¶Ø¹...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_excel('Text_summarization_dataset.xlsx')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16449fd5-89e5-4a4f-8da4-3cb852883e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 20535\n",
      "Validation: 2933\n",
      "Test: 5868\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_data, temp_data = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "\n",
    "val_data, test_data = train_test_split(temp_data, test_size=2/3, random_state=42)\n",
    "\n",
    "\n",
    "print(f\"Train: {len(train_data)}\\nValidation: {len(val_data)}\\nTest: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92de9a05-53a3-4cac-9514-74f032623e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model=model_ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27deb694-c480-4c26-ba9f-3a8397f358c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 256\n",
    "max_target_length = 64\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # ØªØ£ÙƒØ¯ Ø£Ù† ÙƒÙ„ Ø¹Ù†ØµØ± Ù†ØµÙŠ\n",
    "    inputs = [str(text) for text in examples[\"text\"]]\n",
    "    targets = [str(summary) for summary in examples[\"summary\"]]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=64,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "154cd8a8-ed73-4a50-acbb-540bc50fce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['summary', 'text'], dtype='object')\n",
      "Index(['summary', 'text'], dtype='object')\n",
      "Index(['summary', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.rename(columns={\"Column1\": \"summary\", \"Column2\": \"text\"})\n",
    "val_data = val_data.rename(columns={\"Column1\": \"summary\", \"Column2\": \"text\"})\n",
    "test_data = test_data.rename(columns={\"Column1\": \"summary\", \"Column2\": \"text\"})\n",
    "\n",
    "train_data = train_data[[\"summary\", \"text\"]]\n",
    "val_data = val_data[[\"summary\", \"text\"]]\n",
    "test_data = test_data[[\"summary\", \"text\"]]  # Ù‡Ø°Ø§ Ø£ØµÙ„Ø§Ù‹ ØªÙ…Ø§Ù…\n",
    "\n",
    "print(train_data.columns)\n",
    "print(val_data.columns)\n",
    "print(test_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3d8f974-34b6-4b81-975b-39cb2dba4b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ff76588-8090-4ba1-a6fc-8e1175866213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5a57770ac7415f81fa172ad734d80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20535 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1115cd230ed4feaab62d6aafc69d9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2933 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93ae81bd-8981-4dcc-bf6c-3f04f3c7177b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.4\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)  # Ù†ØªØ£ÙƒØ¯ Ø£Ù†Ù‡ 4.30 Ø£Ùˆ Ø£Ø¹Ù„Ù‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04ed0cb5-bca5-43c9-a09b-33b5668aa81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cd650ba-7b4a-4bf6-bc75-079b67197595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Ø¹Ø¯Ù‘Ù„ Ø§Ù„Ù€ training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"no\",  # Ù†ÙˆÙ‚Ù Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,  # ÙƒØ¨Ø¯Ø§ÙŠØ©\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†ØµÙ Ø¯Ù‚Ø© Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96277cd4-7a43-4664-a760-f1fa803c19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97ee20c2-851e-4276-8714-3d8fe203e9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10268' max='10268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10268/10268 9:52:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.538700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.337700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.198800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.102300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.082900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.042900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--csebuetnlp--mT5_multilingual_XLSum. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7d840b4768463b9416b0d17bbf264e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:3465: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 84, 'num_beams': 4, 'length_penalty': 0.6, 'no_repeat_ngram_size': 2}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10268, training_loss=2.1025842087808453, metrics={'train_runtime': 35560.7289, 'train_samples_per_second': 0.577, 'train_steps_per_second': 0.289, 'total_flos': 2.462241231470592e+16, 'train_loss': 2.1025842087808453, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "854adb5f-8973-475c-a0c7-c3fdfb614f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ØªØ«Ø¨ÙŠØª Gradio Ø£ÙˆÙ„ Ù…Ø±Ø©\n",
    "!pip install gradio -q\n",
    "\n",
    "# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨\n",
    "model_path = \"C:/Users/Admin/Downloads/results/checkpoint-10268\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ„Ø®ÙŠØµ\n",
    "def summarize_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True).to(model.device)\n",
    "    summary_ids = model.generate(\n",
    "      **inputs,\n",
    "      max_length=256,\n",
    "      min_length=60,\n",
    "      num_beams=6,\n",
    "      top_k=50,\n",
    "      top_p=0.95,\n",
    "      temperature=0.9,\n",
    "      repetition_penalty=1.2,\n",
    "      no_repeat_ngram_size=3,\n",
    "      early_stopping=True\n",
    "    )\n",
    "\n",
    "\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Ø¨Ù†Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© Gradio\n",
    "interface = gr.Interface(\n",
    "    fn=summarize_text,\n",
    "    inputs=gr.Textbox(lines=10, placeholder=\"Ø§ÙƒØªØ¨ Ø§Ù„Ù†Øµ Ù‡Ù†Ø§...\", label=\"ğŸ“ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„\"),\n",
    "    outputs=gr.Textbox(label=\"âœ‚ï¸ Ø§Ù„Ù…Ù„Ø®Øµ\"),\n",
    "    title=\"Ù…Ù„Ø®Øµ Ù†ØµÙˆØµ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… T5\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1cbedd-1f1e-4ad4-805f-fb1bc9b0b355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
